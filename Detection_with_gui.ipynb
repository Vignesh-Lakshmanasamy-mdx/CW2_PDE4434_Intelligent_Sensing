{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbeb9ee0-257b-4803-b265-0e968fed1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 , math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2404ff4e-6263-44bb-81c0-c453f9c0aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#color detection\n",
    "def detect_card_color(img):\n",
    "    \"\"\" Determining the color of the card using HSV color space \"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # color ranges for UNO colors\n",
    "    colors = {\n",
    "        \"Red\": ([0, 120, 70], [10, 255, 255]),\n",
    "        \"Yellow\": ([20, 100, 100], [30, 255, 255]),\n",
    "        \"Green\": ([35, 100, 100], [85, 255, 255]),\n",
    "        \"Blue\": ([90, 100, 100], [130, 255, 255])\n",
    "    }\n",
    "\n",
    "    for color, (lower, upper) in colors.items():\n",
    "        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n",
    "        if cv2.countNonZero(mask) > 500:  # If enough pixels match the color\n",
    "            return color\n",
    "\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2309d1c-6096-4e24-9688-bb6684e20100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_camera(img):\n",
    "    # Load trained model\n",
    "    model = tf.keras.models.load_model(\"uno_number_detection.keras\")\n",
    "    \n",
    "    # Define categories\n",
    "    categories = [\"Number_1\", \"Number_2\", \"Number_3\",\"Number_4\",\"Number_5\",\"Number_6\",\"Number_7\",\"Number_8\",\"Number_9\",\"Symbol_Draw_2\",\"Symbol_Reverse\",\"Symbol_Skip\"]\n",
    "    \n",
    "       \n",
    "    frame=img.copy()\n",
    "    \n",
    "    img_size=224\n",
    "\n",
    "    #preprocessing cycle -1 :\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)#converting to gray scale\n",
    "    _, thresh = cv2.threshold(gray,200, 255, cv2.THRESH_BINARY) #thresholding the image\n",
    "    edges = cv2.Canny(thresh, 100, 200)# edge detection\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8) #matrix size for dilation and eroding 5 x 5\n",
    "\n",
    "    dilated = cv2.dilate(edges, kernel, iterations=2) #dilation\n",
    "    eroded = cv2.erode(dilated, kernel, iterations=1) #erosion\n",
    "    \n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # finds the contour\n",
    "    \n",
    "    cv2.drawContours(frame, contours, -1, (0, 255, 0), 3) # draw a contour around the uno card\n",
    "    \n",
    "    img_input=frame.copy()\n",
    "    copy_image=frame.copy()\n",
    "    \n",
    "    for contour in contours:\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "\n",
    "        box = cv2.boxPoints(rect) # creating the box points\n",
    "        box = box.astype(int) # getting the x, y value for each cornor points\n",
    "        \n",
    "        cv2.drawContours(img_input, [box], -1, (255, 0, 0), 3) # drawing a box\n",
    "\n",
    "    #calculating the center point\n",
    "    center_x = int(np.mean(box[:, 0]))\n",
    "    center_y = int(np.mean(box[:, 1]))\n",
    "\n",
    "    # Draw the center point\n",
    "    cv2.circle(img_input, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    pt1, pt2, pt3, pt4 = box #four points of the rectangle\n",
    "\n",
    "    # midpoint of pt1 - pt2\n",
    "    mid1 = ((pt1[0] + pt2[0]) // 2, (pt1[1] + pt2[1]) // 2)\n",
    "    \n",
    "    # midpoint of pt3 - pt4\n",
    "    mid2 = ((pt3[0] + pt4[0]) // 2, (pt3[1] + pt4[1]) // 2)\n",
    "    \n",
    "    #midpoint of pt1 - pt4\n",
    "    mid3=((pt1[0] + pt4[0]) // 2, (pt1[1] + pt4[1]) // 2)\n",
    "    \n",
    "    #midpoint of pt2 - pt3\n",
    "    mid4 = ((pt3[0] + pt2[0]) // 2, (pt3[1] + pt2[1]) // 2)\n",
    "\n",
    "    # Draw the line between midpoints\n",
    "    cv2.line(img_input, mid1, mid2, (255, 255, 0), 2)  \n",
    "    cv2.line(img_input, mid3, mid4, (0, 255, 0), 2)\n",
    "    \n",
    "    #distance between points or length of the line\n",
    "    line1_length=math.sqrt((mid2[0] - mid1[0])**2+(mid2[1] - mid1[1])**2)\n",
    "    line2_length=math.sqrt((mid4[0] - mid3[0])**2+(mid4[1] - mid3[1])**2)\n",
    "\n",
    "    #finding the longest line and setting those mid point to take angle\n",
    "    if line1_length>line2_length:\n",
    "        midpoint1=mid1\n",
    "        midpoint2=mid2\n",
    "    else:\n",
    "        midpoint1=mid3\n",
    "        midpoint2=mid4\n",
    "        \n",
    "    # length of reference line\n",
    "    line_length = 100\n",
    "    \n",
    "    # Starting point is center\n",
    "    start_point = (center_x, center_y)\n",
    "    \n",
    "    # End point straight up (90 degrees)\n",
    "    end_point = (center_x, center_y - line_length)\n",
    "    \n",
    "    # Drawing the vertical reference line\n",
    "    cv2.line(img_input, start_point, end_point, (0, 255, 255), 2)  # Yellow line\n",
    "                    \n",
    "    # Vector of midpoint line\n",
    "    vector_x = midpoint2[0] - midpoint1[0]\n",
    "    vector_y = midpoint2[1] - midpoint1[1]\n",
    "    \n",
    "    # Angle between vector and vertical line (0, -1)\n",
    "    angle_rad = np.arctan2(vector_y, vector_x) - np.arctan2(-1, 0)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    angle_deg = angle_deg % 360 \n",
    "                    \n",
    "    # Getting image center\n",
    "    (h, w) = img_input.shape[:2]\n",
    "    image_center = (w // 2, h // 2)\n",
    "    \n",
    "    # rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(image_center, angle_deg, 1.0)\n",
    "    \n",
    "    # Perform the rotation\n",
    "    rotated = cv2.warpAffine(copy_image, M, (w, h))\n",
    "    #show_image(rotated, \"Rotated image\")\n",
    "    \n",
    "    # preprocessing - cycle2\n",
    "\n",
    "    gray_2 = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY) \n",
    "    _, thresh_2 = cv2.threshold(gray_2,220, 255, cv2.THRESH_BINARY)             \n",
    "    edges_2 = cv2.Canny(thresh_2, 100, 200)\n",
    "                  \n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    dilated_2 = cv2.dilate(edges_2, kernel, iterations=2)\n",
    "    eroded_2 = cv2.erode(dilated_2, kernel, iterations=1)\n",
    "    \n",
    "    contours, _ = cv2.findContours(eroded_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Process only if we have at least one valid contour\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(rotated, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "        \n",
    "        \n",
    "        if area > 500:  # Filter small areas\n",
    "                                   \n",
    "            cropped = rotated[y:y+h, x:x+w]                       \n",
    "            resize_img = cv2.resize(cropped, (224, 224))\n",
    "            \n",
    "            # preprocessing - cycle3\n",
    "            \n",
    "            gray_3 = cv2.cvtColor(resize_img, cv2.COLOR_BGR2GRAY)                       \n",
    "            _, thresh_3 = cv2.threshold(gray_3,220, 255, cv2.THRESH_BINARY)\n",
    "            edges_3 = cv2.Canny(thresh_3, 100, 200)\n",
    "            \n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            \n",
    "            dilated_3 = cv2.dilate(edges_3, kernel, iterations=2)\n",
    "            eroded_3 = cv2.erode(dilated_3, kernel, iterations=1)\n",
    "            \n",
    "            contours, _ = cv2.findContours(thresh_3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for cnt in contours:\n",
    "                area2 = cv2.contourArea(cnt)\n",
    "                \n",
    "                \n",
    "                if area2> 19000 and area2 <30000:\n",
    "                                        \n",
    "                    background = np.ones((224, 224, 3), dtype=np.uint8) * 255  \n",
    "                    \n",
    "                    # Creating mask for the oval contour\n",
    "                    mask = np.zeros_like(resize_img)\n",
    "                    cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                                                   \n",
    "                    # Applying mask to extract oval region\n",
    "                    masked = cv2.bitwise_and(resize_img, mask)\n",
    "                    \n",
    "                    # Crop the contour region\n",
    "                    x, y, w, h = cv2.boundingRect(cnt)\n",
    "                    cropped = masked[y:y+h, x:x+w]\n",
    "                    \n",
    "                    # Get shape of cropped region\n",
    "                    ch, cw = cropped.shape[:2]\n",
    "    \n",
    "                    target_size = 224\n",
    "                    # Compute offsets to center it\n",
    "                    x_offset = (target_size - cw) // 2\n",
    "                    y_offset = (target_size - ch) // 2\n",
    "                    \n",
    "                    # Pasting cropped oval into the center of white background\n",
    "                    background[y_offset:y_offset+ch, x_offset:x_offset+cw] = cropped\n",
    "    \n",
    "                    #final_crop=cv2.cvtColor(background, cv2.COLOR_BGR2RGB)\n",
    "                    gray_final = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "                    _, thresh_final = cv2.threshold(gray_final,226, 255, cv2.THRESH_BINARY)\n",
    "                    #show_image(thresh_final,\"final - image\",cmap=\"grey\")\n",
    "\n",
    "                    # Resize to final model input size\n",
    "                    final_img = cv2.resize(thresh_final, (img_size, img_size))\n",
    "                    \"\"\"\n",
    "                    # Preprocess the image\n",
    "                    img_processed = preprocess_input(final_img.astype(np.float32))\n",
    "                    img_array = np.expand_dims(img_processed, axis=0)\n",
    "\n",
    "                    predictions = model.predict(img_array, verbose=0)\n",
    "                    class_id = np.argmax(predictions)\n",
    "                    confidence = np.max(predictions)\n",
    "\n",
    "                    label = f\"{categories[class_id]}\"\n",
    "\n",
    "                    color = detect_card_color(background)\n",
    "\n",
    "                #cv2.putText(frame,\"Label -\"+label,(10, 30),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "                #cv2.putText(frame,\"Color -\"+color,(30, 60),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "                \"\"\"\n",
    "                \n",
    "    return frame,final_img,background\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35016f7-18f8-46df-aafb-ba0c69822421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form implementation generated from reading ui file 'gui.ui'\n",
    "#\n",
    "# Created by: PyQt6 UI code generator 6.4.2\n",
    "#\n",
    "# WARNING: Any manual changes made to this file will be lost when pyuic6 is\n",
    "# run again.  Do not edit this file unless you know what you are doing.\n",
    "\n",
    "\n",
    "from PyQt6 import QtCore, QtGui, QtWidgets\n",
    "from PyQt6.QtWidgets import QFileDialog\n",
    "\n",
    "\n",
    "    \n",
    "class Ui_Dialog(object):\n",
    "    def setupUi(self, Dialog):\n",
    "        Dialog.setObjectName(\"Dialog\")\n",
    "        Dialog.resize(640, 480)\n",
    "        self.buttonBox = QtWidgets.QDialogButtonBox(parent=Dialog)\n",
    "        self.buttonBox.setGeometry(QtCore.QRect(10, 440, 621, 32))\n",
    "        self.buttonBox.setOrientation(QtCore.Qt.Orientation.Horizontal)\n",
    "        self.buttonBox.setStandardButtons(QtWidgets.QDialogButtonBox.StandardButton.Cancel|QtWidgets.QDialogButtonBox.StandardButton.Ok)\n",
    "        self.buttonBox.setObjectName(\"buttonBox\")\n",
    "        \n",
    "        self.textBrowser = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.textBrowser.setGeometry(QtCore.QRect(20, 20, 601, 111))\n",
    "        self.textBrowser.setObjectName(\"textBrowser\")\n",
    "        \n",
    "        self.textEdit = QtWidgets.QTextEdit(parent=Dialog)\n",
    "        self.textEdit.setGeometry(QtCore.QRect(20, 140, 291, 291))\n",
    "        self.textEdit.setObjectName(\"textEdit\")\n",
    "        \n",
    "        self.camera_number = QtWidgets.QSpinBox(parent=Dialog)\n",
    "        self.camera_number.setGeometry(QtCore.QRect(180, 190, 121, 41))\n",
    "        self.camera_number.setMaximum(2)\n",
    "        self.camera_number.setObjectName(\"camera_number\") #camera index number\n",
    "        \n",
    "        self.textBrowser_2 = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.textBrowser_2.setGeometry(QtCore.QRect(30, 190, 131, 41))\n",
    "        self.textBrowser_2.setObjectName(\"textBrowser_2\")\n",
    "        \n",
    "        self.Live_start = QtWidgets.QPushButton(parent=Dialog)\n",
    "        self.Live_start.setGeometry(QtCore.QRect(30, 240, 131, 41))\n",
    "        self.Live_start.setObjectName(\"Live_start\") #start camera\n",
    "        \n",
    "        self.Live_stop = QtWidgets.QPushButton(parent=Dialog)\n",
    "        self.Live_stop.setGeometry(QtCore.QRect(180, 240, 121, 41))\n",
    "        self.Live_stop.setObjectName(\"Live_stop\") #stop camera\n",
    "        \n",
    "        self.textEdit_2 = QtWidgets.QTextEdit(parent=Dialog)\n",
    "        self.textEdit_2.setGeometry(QtCore.QRect(330, 140, 291, 291))\n",
    "        self.textEdit_2.setObjectName(\"textEdit_2\")\n",
    "        \n",
    "        self.Imagepredict = QtWidgets.QPushButton(parent=Dialog)\n",
    "        self.Imagepredict.setGeometry(QtCore.QRect(490, 260, 111, 41))\n",
    "        self.Imagepredict.setObjectName(\"Imagepredict\") #image predict button\n",
    "        \n",
    "        self.textBrowser_3 = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.textBrowser_3.setGeometry(QtCore.QRect(340, 190, 261, 61))\n",
    "        self.textBrowser_3.setObjectName(\"textBrowser_3\")\n",
    "\n",
    "        \"\"\"\n",
    "        self.FileNumberinput = QtWidgets.QTextEdit(parent=Dialog)\n",
    "        self.FileNumberinput.setGeometry(QtCore.QRect(340, 260, 141, 41))\n",
    "        font = QtGui.QFont()\n",
    "        font.setFamily(\"URW Bookman\")\n",
    "        font.setPointSize(11)\n",
    "        font.setBold(True)\n",
    "        self.FileNumberinput.setFont(font)\n",
    "        self.FileNumberinput.setObjectName(\"FileNumberinput\") # File name as number input\n",
    "        \"\"\"\n",
    "        self.Live_predictionoutput = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.Live_predictionoutput.setGeometry(QtCore.QRect(40, 320, 251, 91))\n",
    "        self.Live_predictionoutput.setObjectName(\"Live_predictionoutput\") # live predict output\n",
    "        \n",
    "        self.label = QtWidgets.QLabel(parent=Dialog)\n",
    "        self.label.setGeometry(QtCore.QRect(40, 291, 251, 31))\n",
    "        self.label.setObjectName(\"label\")\n",
    "        self.Imagepredictionoutput = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.Imagepredictionoutput.setGeometry(QtCore.QRect(340, 329, 261, 91))\n",
    "        self.Imagepredictionoutput.setObjectName(\"Imagepredictionoutput\") # image predict output\n",
    "        \n",
    "        self.Label1 = QtWidgets.QLabel(parent=Dialog)\n",
    "        self.Label1.setGeometry(QtCore.QRect(340, 300, 261, 31))\n",
    "        self.Label1.setObjectName(\"Label1\")\n",
    "        self.textBrowser_4 = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.textBrowser_4.setGeometry(QtCore.QRect(30, 140, 271, 41))\n",
    "        self.textBrowser_4.setObjectName(\"textBrowser_4\")\n",
    "        self.textBrowser_5 = QtWidgets.QTextBrowser(parent=Dialog)\n",
    "        self.textBrowser_5.setGeometry(QtCore.QRect(340, 140, 271, 41))\n",
    "        self.textBrowser_5.setObjectName(\"textBrowser_5\")\n",
    "        self.textEdit_2.raise_()\n",
    "        self.buttonBox.raise_()\n",
    "        self.textBrowser.raise_()\n",
    "        self.textEdit.raise_()\n",
    "        self.camera_number.raise_()\n",
    "        self.textBrowser_2.raise_()\n",
    "        self.Live_start.raise_()\n",
    "        self.Live_stop.raise_()\n",
    "        self.Imagepredict.raise_()\n",
    "        self.textBrowser_3.raise_()\n",
    "        #self.FileNumberinput.raise_()\n",
    "        self.Live_predictionoutput.raise_()\n",
    "        self.label.raise_()\n",
    "        self.Imagepredictionoutput.raise_()\n",
    "        self.Label1.raise_()\n",
    "        self.textBrowser_4.raise_()\n",
    "        self.textBrowser_5.raise_()\n",
    "\n",
    "        self.retranslateUi(Dialog)\n",
    "        self.buttonBox.accepted.connect(Dialog.accept) # type: ignore\n",
    "        self.buttonBox.rejected.connect(Dialog.reject) # type: ignore\n",
    "        self.Imagepredict.clicked.connect(self.predict_from_file)\n",
    "        \n",
    "        self.Live_start.clicked.connect(self.start_camera)\n",
    "        self.Live_stop.clicked.connect(self.stop_camera)\n",
    "\n",
    "        self.cap = None\n",
    "        self.timer = QtCore.QTimer()\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        \n",
    "        QtCore.QMetaObject.connectSlotsByName(Dialog)\n",
    "        \n",
    "\n",
    "    #program to run the camera and detect\n",
    "    def start_camera(self):\n",
    "        camera_index = self.camera_number.value()\n",
    "        self.cap = cv2.VideoCapture(camera_index)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        self.timer.start(30)\n",
    "\n",
    "    def update_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            processed_frame,imageforprediction,imageforcolor = live_camera(frame)\n",
    "\n",
    "             # Load trained model\n",
    "            model = tf.keras.models.load_model(\"uno_number_detection.keras\")\n",
    "    \n",
    "            # Define categories\n",
    "            categories = [\"Number_1\", \"Number_2\", \"Number_3\",\"Number_4\",\"Number_5\",\"Number_6\",\"Number_7\",\"Number_8\",\"Number_9\",\"Symbol_Draw_2\",\"Symbol_Reverse\",\"Symbol_Skip\"]\n",
    "            \n",
    "            #self.Live_predictionoutput.setText(\"Live prediction:\")\n",
    "            # Preprocess the image\n",
    "            img_processed = preprocess_input(imageforprediction.astype(np.float32))\n",
    "            img_array = np.expand_dims(img_processed, axis=0)\n",
    "\n",
    "            predictions = model.predict(img_array, verbose=0)\n",
    "            class_id = np.argmax(predictions)\n",
    "            confidence = np.max(predictions)\n",
    "\n",
    "            label = f\"{categories[class_id]}\"\n",
    "\n",
    "            color = detect_card_color(imageforcolor)\n",
    "\n",
    "            cv2.putText(processed_frame,\"Label -\"+label,(10, 30),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "            cv2.putText(processed_frame,\"Color -\"+color,(30, 60),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "            \n",
    "            self.Live_predictionoutput.setText(label+\"\\n\" + color)\n",
    "            \n",
    "            cv2.imshow(\"Live Detection\", processed_frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    def stop_camera(self):\n",
    "        self.timer.stop()\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def predict_from_file(self):\n",
    "        file_path, _ = QFileDialog.getOpenFileName(None, \"Select Image\", \"\", \"Image Files (*.png *.jpg *.jpeg *.bmp)\") #took it from google\n",
    "        if file_path:\n",
    "            image = cv2.imread(file_path)\n",
    "            processed_frame, imageforprediction, imageforcolor = live_camera(image)\n",
    "            \n",
    "            # Load trained model\n",
    "            model = tf.keras.models.load_model(\"uno_number_detection.keras\")\n",
    "    \n",
    "            # Define categories\n",
    "            categories = [\"Number_1\", \"Number_2\", \"Number_3\",\"Number_4\",\"Number_5\",\"Number_6\",\"Number_7\",\"Number_8\",\"Number_9\",\"Symbol_Draw_2\",\"Symbol_Reverse\",\"Symbol_Skip\"]\n",
    "            \n",
    "            #self.Live_predictionoutput.setText(\"Live prediction:\")\n",
    "            # Preprocess the image\n",
    "            img_processed = preprocess_input(imageforprediction.astype(np.float32))\n",
    "            img_array = np.expand_dims(img_processed, axis=0)\n",
    "\n",
    "            predictions = model.predict(img_array, verbose=0)\n",
    "            class_id = np.argmax(predictions)\n",
    "            confidence = np.max(predictions)\n",
    "\n",
    "            label = f\"{categories[class_id]}\"\n",
    "\n",
    "            color = detect_card_color(imageforcolor)\n",
    "\n",
    "            cv2.putText(processed_frame,\"Label -\"+label,(10, 30),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "            cv2.putText(processed_frame,\"Color -\"+color,(30, 60),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),1,cv2.LINE_AA)\n",
    "            \n",
    "            self.Imagepredictionoutput.setText(label+\"\\n\" + color)\n",
    "            \n",
    "            cv2.imshow(\"Image prediction output\", processed_frame)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "    def retranslateUi(self, Dialog):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        Dialog.setWindowTitle(_translate(\"Dialog\", \"Dialog\"))\n",
    "        self.textBrowser.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><span style=\\\" font-family:\\'Ubuntu\\'; font-size:11pt; font-weight:600;\\\">PDE4434 - IntelligentSensing</span></p>\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px; font-family:\\'Ubuntu\\'; font-size:11pt; font-weight:600;\\\"><br /></p>\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><span style=\\\" font-family:\\'Ubuntu\\'; font-size:11pt;\\\">Coursework 2</span></p>\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px; font-family:\\'Ubuntu\\'; font-size:11pt;\\\"><br /></p>\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><span style=\\\" font-family:\\'Ubuntu\\'; font-size:11pt; font-weight:600; color:#204a87;\\\">Uno Card Color and Number Detection </span></p>\\n\"\n",
    "\"<p style=\\\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px; font-family:\\'Ubuntu\\'; font-size:11pt;\\\"><br /></p></body></html>\"))\n",
    "        self.textEdit.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><br /></p></body></html>\"))\n",
    "        self.textBrowser_2.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\">Camera Index</p></body></html>\"))\n",
    "        self.Live_start.setText(_translate(\"Dialog\", \"Start\"))\n",
    "        self.Live_stop.setText(_translate(\"Dialog\", \"Stop\"))\n",
    "        self.textEdit_2.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\"-qt-paragraph-type:empty; margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><br /></p></body></html>\"))\n",
    "        self.Imagepredict.setText(_translate(\"Dialog\", \"Predict\"))\n",
    "        self.textBrowser_3.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\">File name in number</p>\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\">(1 to 30)</p></body></html>\"))\n",
    "        self.label.setText(_translate(\"Dialog\", \"Detected Card :\"))\n",
    "        self.Label1.setText(_translate(\"Dialog\", \"Detected Card :\"))\n",
    "        self.textBrowser_4.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p align=\\\"center\\\" style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><span style=\\\" font-size:14pt; font-weight:700; color:#ff0000;\\\">Live Camera Detection</span></p></body></html>\"))\n",
    "        self.textBrowser_5.setHtml(_translate(\"Dialog\", \"<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\\n\"\n",
    "\"<html><head><meta name=\\\"qrichtext\\\" content=\\\"1\\\" /><meta charset=\\\"utf-8\\\" /><style type=\\\"text/css\\\">\\n\"\n",
    "\"p, li { white-space: pre-wrap; }\\n\"\n",
    "\"hr { height: 1px; border-width: 0; }\\n\"\n",
    "\"li.unchecked::marker { content: \\\"\\\\2610\\\"; }\\n\"\n",
    "\"li.checked::marker { content: \\\"\\\\2612\\\"; }\\n\"\n",
    "\"</style></head><body style=\\\" font-family:\\'Segoe UI\\'; font-size:12.75pt; font-weight:400; font-style:normal;\\\">\\n\"\n",
    "\"<p style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\"><span style=\\\" font-size:14pt; font-weight:700; color:#00007f;\\\">Testing images from dataset</span></p></body></html>\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    Dialog = QtWidgets.QDialog()\n",
    "    ui = Ui_Dialog()\n",
    "    ui.setupUi(Dialog)\n",
    "    Dialog.show()\n",
    "    sys.exit(app.exec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351d158-86fe-4a6a-8e70-b5dde45e4dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47529b10-db89-48bd-82c6-5ccea69e04ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
