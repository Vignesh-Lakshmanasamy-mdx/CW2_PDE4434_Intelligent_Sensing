{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe8bf6a-d81b-49a2-a00c-3c8413284d19",
   "metadata": {},
   "source": [
    "Step 1 : Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdc26b8-578c-43d4-96e0-5e05a5562e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import os\n",
    "import cv2 , math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed564673-a157-42bd-8e4f-3730bfdda7e5",
   "metadata": {},
   "source": [
    "Step 2 : Routing the path of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22289b6-2335-4145-b4a0-52798e6941f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: C:\\Users\\vigne\\Desktop\\CW2_PDE4434_Intelligent_Sensing\\CW2_PDE4434_Intelligent_Sensing\\Dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# use current working directory\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"Dataset\"\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "categories = [\"Number_1\", \"Number_2\", \"Number_3\",\"Number_4\",\"Number_5\",\"Number_6\",\"Number_7\",\"Number_8\",\"Number_9\",\"Symbol_Draw_2\",\"Symbol_Reverse\",\"Symbol_Skip\"]\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7d11a-2c17-4e8c-b7bd-7c5df6834b8d",
   "metadata": {},
   "source": [
    "Step 3 : Loading the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b5babaf-e1bf-4c0f-9186-561ed4fe4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(data_dir, category)\n",
    "        label = categories.index(category)  \n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_input = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "                #copying the origianl image\n",
    "                copy_image=img_input.copy()\n",
    "\n",
    "                #preprocessing cycle -1 :\n",
    "                gray = cv2.cvtColor(img_input, cv2.COLOR_BGR2GRAY)#converting to gray scale\n",
    "                _, thresh = cv2.threshold(gray,200, 255, cv2.THRESH_BINARY) #thresholding the image\n",
    "                edges = cv2.Canny(thresh, 100, 200)# edge detection\n",
    "\n",
    "                kernel = np.ones((5, 5), np.uint8) #matrix size for dilation and eroding 5 x 5\n",
    "\n",
    "                dilated = cv2.dilate(edges, kernel, iterations=2) #dilation\n",
    "                eroded = cv2.erode(dilated, kernel, iterations=1) #erosion\n",
    "                \n",
    "                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # finds the contour\n",
    "                cv2.drawContours(img_input, contours, -1, (0, 255, 0), 3) # draw a contour around the uno card\n",
    "\n",
    "                for contour in contours:\n",
    "                    rect = cv2.minAreaRect(contour)\n",
    "\n",
    "                    box = cv2.boxPoints(rect) # creating the box points\n",
    "                    box = box.astype(int) # getting the x, y value for each cornor points\n",
    "                    \n",
    "                    cv2.drawContours(img_input, [box], -1, (255, 0, 0), 3) # drawing a box\n",
    "\n",
    "                #calculating the center point\n",
    "                center_x = int(np.mean(box[:, 0]))\n",
    "                center_y = int(np.mean(box[:, 1]))\n",
    "\n",
    "                # Draw the center point\n",
    "                cv2.circle(img_input, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "                pt1, pt2, pt3, pt4 = box #four points of the rectangle\n",
    "\n",
    "                # midpoint of pt1 - pt2\n",
    "                mid1 = ((pt1[0] + pt2[0]) // 2, (pt1[1] + pt2[1]) // 2)\n",
    "                \n",
    "                # midpoint of pt3 - pt4\n",
    "                mid2 = ((pt3[0] + pt4[0]) // 2, (pt3[1] + pt4[1]) // 2)\n",
    "                \n",
    "                #midpoint of pt1 - pt4\n",
    "                mid3=((pt1[0] + pt4[0]) // 2, (pt1[1] + pt4[1]) // 2)\n",
    "                \n",
    "                #midpoint of pt2 - pt3\n",
    "                mid4 = ((pt3[0] + pt2[0]) // 2, (pt3[1] + pt2[1]) // 2)\n",
    "\n",
    "                # Draw the line between midpoints\n",
    "                cv2.line(img_input, mid1, mid2, (255, 255, 0), 2)  \n",
    "                cv2.line(img_input, mid3, mid4, (0, 255, 0), 2)\n",
    "                \n",
    "                #distance between points or length of the line\n",
    "                line1_length=math.sqrt((mid2[0] - mid1[0])**2+(mid2[1] - mid1[1])**2)\n",
    "                line2_length=math.sqrt((mid4[0] - mid3[0])**2+(mid4[1] - mid3[1])**2)\n",
    "\n",
    "                #finding the longest line and setting those mid point to take angle\n",
    "                if line1_length>line2_length:\n",
    "                    midpoint1=mid1\n",
    "                    midpoint2=mid2\n",
    "                else:\n",
    "                    midpoint1=mid3\n",
    "                    midpoint2=mid4\n",
    "                    \n",
    "                # length of reference line\n",
    "                line_length = 100\n",
    "                \n",
    "                # Starting point is center\n",
    "                start_point = (center_x, center_y)\n",
    "                \n",
    "                # End point straight up (90 degrees)\n",
    "                end_point = (center_x, center_y - line_length)\n",
    "                \n",
    "                # Drawing the vertical reference line\n",
    "                cv2.line(img_input, start_point, end_point, (0, 255, 255), 2)  # Yellow line\n",
    "                                \n",
    "                # Vector of midpoint line\n",
    "                vector_x = midpoint2[0] - midpoint1[0]\n",
    "                vector_y = midpoint2[1] - midpoint1[1]\n",
    "                \n",
    "                # Angle between vector and vertical line (0, -1)\n",
    "                angle_rad = np.arctan2(vector_y, vector_x) - np.arctan2(-1, 0)\n",
    "                angle_deg = np.degrees(angle_rad)\n",
    "                angle_deg = angle_deg % 360 \n",
    "                                \n",
    "                # Getting image center\n",
    "                (h, w) = img_input.shape[:2]\n",
    "                image_center = (w // 2, h // 2)\n",
    "                \n",
    "                # rotation matrix\n",
    "                M = cv2.getRotationMatrix2D(image_center, angle_deg, 1.0)\n",
    "                \n",
    "                # Perform the rotation\n",
    "                rotated = cv2.warpAffine(copy_image, M, (w, h))\n",
    "                #show_image(rotated, \"Rotated image\")\n",
    "                \n",
    "                # preprocessing - cycle2\n",
    "\n",
    "                gray_2 = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY) \n",
    "                _, thresh_2 = cv2.threshold(gray_2,220, 255, cv2.THRESH_BINARY)             \n",
    "                edges_2 = cv2.Canny(thresh_2, 100, 200)\n",
    "                              \n",
    "                kernel = np.ones((5, 5), np.uint8)\n",
    "                \n",
    "                dilated_2 = cv2.dilate(edges_2, kernel, iterations=2)\n",
    "                eroded_2 = cv2.erode(dilated_2, kernel, iterations=1)\n",
    "                \n",
    "                contours, _ = cv2.findContours(eroded_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "                # Process only if we have at least one valid contour\n",
    "                for cnt in contours:\n",
    "                    area = cv2.contourArea(cnt)\n",
    "                    x, y, w, h = cv2.boundingRect(cnt)\n",
    "                    cv2.rectangle(rotated, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "                    \n",
    "                    \n",
    "                    if area > 500:  # Filter small areas\n",
    "                                               \n",
    "                        cropped = rotated[y:y+h, x:x+w]                       \n",
    "                        resize_img = cv2.resize(cropped, (224, 224))\n",
    "                        \n",
    "                        # preprocessing - cycle3\n",
    "                        \n",
    "                        gray_3 = cv2.cvtColor(resize_img, cv2.COLOR_BGR2GRAY)                       \n",
    "                        _, thresh_3 = cv2.threshold(gray_3,220, 255, cv2.THRESH_BINARY)\n",
    "                        edges_3 = cv2.Canny(thresh_3, 100, 200)\n",
    "                        \n",
    "                        kernel = np.ones((5, 5), np.uint8)\n",
    "                        \n",
    "                        dilated_3 = cv2.dilate(edges_3, kernel, iterations=2)\n",
    "                        eroded_3 = cv2.erode(dilated_3, kernel, iterations=1)\n",
    "                        \n",
    "                        contours, _ = cv2.findContours(thresh_3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "                        for cnt in contours:\n",
    "                            area2 = cv2.contourArea(cnt)\n",
    "                            \n",
    "                            \n",
    "                            if area2> 19000 and area2 <30000:\n",
    "                                                    \n",
    "                                background = np.ones((224, 224, 3), dtype=np.uint8) * 255  \n",
    "                                \n",
    "                                # Creating mask for the oval contour\n",
    "                                mask = np.zeros_like(resize_img)\n",
    "                                cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                                                               \n",
    "                                # Applying mask to extract oval region\n",
    "                                masked = cv2.bitwise_and(resize_img, mask)\n",
    "                                \n",
    "                                # Crop the contour region\n",
    "                                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                                cropped = masked[y:y+h, x:x+w]\n",
    "                                \n",
    "                                # Get shape of cropped region\n",
    "                                ch, cw = cropped.shape[:2]\n",
    "                \n",
    "                                target_size = 224\n",
    "                                # Compute offsets to center it\n",
    "                                x_offset = (target_size - cw) // 2\n",
    "                                y_offset = (target_size - ch) // 2\n",
    "                                \n",
    "                                # Pasting cropped oval into the center of white background\n",
    "                                background[y_offset:y_offset+ch, x_offset:x_offset+cw] = cropped\n",
    "                \n",
    "                                #final_crop=cv2.cvtColor(background, cv2.COLOR_BGR2RGB)\n",
    "                                gray_final = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "                                _, thresh_final = cv2.threshold(gray_final,226, 255, cv2.THRESH_BINARY)\n",
    "                                #show_image(thresh_final,\"final - image\",cmap=\"grey\")\n",
    "                                \n",
    "                                data.append(thresh_final)\n",
    "                                labels.append(label)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {e}\")\n",
    "    return np.array(data), np.array(labels)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ced532-eec6-4149-b299-e8dd664eab67",
   "metadata": {},
   "source": [
    "Step 4 : Loading the data and reshaping to prepare for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a9f0b60-d31c-4337-a984-930818e9a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 224, 224, 1) (209, 12)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data()\n",
    "X = X.reshape(-1, img_size, img_size, 1) / 255.0  \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(categories)) \n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a056049-0173-4787-8670-51dac09e8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=\"Image\", cmap=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66076a73-736c-4dda-b611-cf1b9735c89f",
   "metadata": {},
   "source": [
    "Step 5 : Spliting the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19741422-996a-46f6-a4f4-6d8b233578f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210a08b-43db-4aea-9d90-8707d00c2503",
   "metadata": {},
   "source": [
    "Step 6 : Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ba67a2b-d78f-4b1c-8f4e-e7c556266388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(img_size, img_size, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(12, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d38857-c395-4838-9f60-63b89c784f54",
   "metadata": {},
   "source": [
    "Step 7 : Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1597f223-d385-4d7d-b509-a14281803dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d90bf6-9dbb-4032-8980-dc259a006c78",
   "metadata": {},
   "source": [
    "Step 8 : Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc5ec22e-3b42-442c-b14e-9058bc2c14d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 234ms/step - accuracy: 0.1062 - loss: 6.6817 - val_accuracy: 0.1429 - val_loss: 2.4416\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - accuracy: 0.2634 - loss: 2.3682 - val_accuracy: 0.6190 - val_loss: 2.3016\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - accuracy: 0.5991 - loss: 1.9693 - val_accuracy: 0.8095 - val_loss: 1.4860\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - accuracy: 0.6817 - loss: 1.1068 - val_accuracy: 0.7857 - val_loss: 0.9090\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.8749 - loss: 0.4495 - val_accuracy: 0.9048 - val_loss: 0.6555\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - accuracy: 0.9646 - loss: 0.1883 - val_accuracy: 0.9286 - val_loss: 0.6612\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - accuracy: 0.9775 - loss: 0.1378 - val_accuracy: 0.9286 - val_loss: 0.6455\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - accuracy: 0.9604 - loss: 0.0940 - val_accuracy: 0.9048 - val_loss: 0.6314\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - accuracy: 0.9774 - loss: 0.1484 - val_accuracy: 0.8810 - val_loss: 0.7263\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 0.0357 - val_accuracy: 0.9286 - val_loss: 0.6731\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - accuracy: 0.9936 - loss: 0.0407 - val_accuracy: 0.9286 - val_loss: 0.7438\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - accuracy: 0.9862 - loss: 0.0875 - val_accuracy: 0.9048 - val_loss: 0.8504\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - accuracy: 0.9760 - loss: 0.0404 - val_accuracy: 0.8810 - val_loss: 0.8933\n"
     ]
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    epochs=100, batch_size=16, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838597c-1f7b-45af-831e-0556a5130148",
   "metadata": {},
   "source": [
    "Step 9 : Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1014d446-aa77-4e65-81bc-a1606fdff530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9261 - loss: 0.4968\n",
      "Test Accuracy: 90.48%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05999d93-779b-4850-b6c7-b100663019d5",
   "metadata": {},
   "source": [
    "Step 10 : Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4e5968f-cc8d-45ba-85f9-5b0781f27edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"uno_number_detection.keras\",include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e9171-cace-4255-b777-3a746befba8a",
   "metadata": {},
   "source": [
    "Step 11 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef30aa8-058e-40b9-af08-9f3ac891b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_image(img_input):\n",
    "    \n",
    "    img_size=224\n",
    "    image_list=[]\n",
    "    input_copy = img_input.copy()\n",
    "\n",
    "    gray_2 = cv2.cvtColor(img_input, cv2.COLOR_BGR2GRAY) \n",
    "    _, thresh_2 = cv2.threshold(gray_2,220, 255, cv2.THRESH_BINARY)             \n",
    "    edges_2 = cv2.Canny(thresh_2, 100, 200)\n",
    "                  \n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    dilated_2 = cv2.dilate(edges_2, kernel, iterations=2)\n",
    "    eroded_2 = cv2.erode(dilated_2, kernel, iterations=1)\n",
    "    \n",
    "    contours, _ = cv2.findContours(eroded_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Process only if we have at least one valid contour\n",
    "    for cnt in sorted_contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(input_copy, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "        \n",
    "        \n",
    "        if area > 2500:  # Filter small areas\n",
    "                                   \n",
    "            cropped = input_copy[y:y+h, x:x+w]                       \n",
    "            image_list.append(cropped)\n",
    "            \n",
    "\n",
    "    return image_list,input_copy\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abc84e-3ebd-4e37-994d-7dae9d03e7b2",
   "metadata": {},
   "source": [
    "Step 12 : Live detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00d34ed7-8630-4df9-9e74-8f69072e455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"uno_number_detection.keras\")\n",
    "\n",
    "# Define categories\n",
    "categories = [\"Number_1\", \"Number_2\", \"Number_3\",\"Number_4\",\"Number_5\",\"Number_6\",\"Number_7\",\"Number_8\",\"Number_9\",\"Symbol_Draw_2\",\"Symbol_Reverse\",\"Symbol_Skip\"]\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set frame width and height\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_size=224\n",
    "    #copying the origianl image\n",
    "    list,contour_image=first_image(frame)\n",
    "    for images in list:\n",
    "            \n",
    "        img_input=images.copy()\n",
    "        copy_image=img_input.copy()\n",
    "    \n",
    "        #preprocessing cycle -1 :\n",
    "        gray = cv2.cvtColor(img_input, cv2.COLOR_BGR2GRAY)#converting to gray scale\n",
    "        _, thresh = cv2.threshold(gray,200, 255, cv2.THRESH_BINARY) #thresholding the image\n",
    "        edges = cv2.Canny(thresh, 100, 200)# edge detection\n",
    "    \n",
    "        kernel = np.ones((5, 5), np.uint8) #matrix size for dilation and eroding 5 x 5\n",
    "    \n",
    "        dilated = cv2.dilate(edges, kernel, iterations=2) #dilation\n",
    "        eroded = cv2.erode(dilated, kernel, iterations=1) #erosion\n",
    "        \n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # finds the contour\n",
    "        cv2.drawContours(img_input, contours, -1, (0, 255, 0), 3) # draw a contour around the uno card\n",
    "        only_contour=img_input.copy()\n",
    "        \n",
    "        for contour in contours:\n",
    "            rect = cv2.minAreaRect(contour)\n",
    "    \n",
    "            box = cv2.boxPoints(rect) # creating the box points\n",
    "            box = box.astype(int) # getting the x, y value for each cornor points\n",
    "            \n",
    "            cv2.drawContours(img_input, [box], -1, (255, 0, 0), 3) # drawing a box\n",
    "    \n",
    "            #calculating the center point\n",
    "            center_x = int(np.mean(box[:, 0]))\n",
    "            center_y = int(np.mean(box[:, 1]))\n",
    "        \n",
    "            # Draw the center point\n",
    "            cv2.circle(img_input, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "        \n",
    "            pt1, pt2, pt3, pt4 = box #four points of the rectangle\n",
    "        \n",
    "            # midpoint of pt1 - pt2\n",
    "            mid1 = ((pt1[0] + pt2[0]) // 2, (pt1[1] + pt2[1]) // 2)\n",
    "            \n",
    "            # midpoint of pt3 - pt4\n",
    "            mid2 = ((pt3[0] + pt4[0]) // 2, (pt3[1] + pt4[1]) // 2)\n",
    "            \n",
    "            #midpoint of pt1 - pt4\n",
    "            mid3=((pt1[0] + pt4[0]) // 2, (pt1[1] + pt4[1]) // 2)\n",
    "            \n",
    "            #midpoint of pt2 - pt3\n",
    "            mid4 = ((pt3[0] + pt2[0]) // 2, (pt3[1] + pt2[1]) // 2)\n",
    "        \n",
    "            # Draw the line between midpoints\n",
    "            cv2.line(img_input, mid1, mid2, (255, 255, 0), 2)  \n",
    "            cv2.line(img_input, mid3, mid4, (0, 255, 0), 2)\n",
    "            \n",
    "            #distance between points or length of the line\n",
    "            line1_length=math.sqrt((mid2[0] - mid1[0])**2+(mid2[1] - mid1[1])**2)\n",
    "            line2_length=math.sqrt((mid4[0] - mid3[0])**2+(mid4[1] - mid3[1])**2)\n",
    "        \n",
    "            #finding the longest line and setting those mid point to take angle\n",
    "            if line1_length>line2_length:\n",
    "                midpoint1=mid1\n",
    "                midpoint2=mid2\n",
    "            else:\n",
    "                midpoint1=mid3\n",
    "                midpoint2=mid4\n",
    "                \n",
    "            # length of reference line\n",
    "            line_length = 100\n",
    "            \n",
    "            # Starting point is center\n",
    "            start_point = (center_x, center_y)\n",
    "            \n",
    "            # End point straight up (90 degrees)\n",
    "            end_point = (center_x, center_y - line_length)\n",
    "            \n",
    "            # Drawing the vertical reference line\n",
    "            cv2.line(img_input, start_point, end_point, (0, 255, 255), 2)  # Yellow line\n",
    "                            \n",
    "            # Vector of midpoint line\n",
    "            vector_x = midpoint2[0] - midpoint1[0]\n",
    "            vector_y = midpoint2[1] - midpoint1[1]\n",
    "            \n",
    "            # Angle between vector and vertical line (0, -1)\n",
    "            angle_rad = np.arctan2(vector_y, vector_x) - np.arctan2(-1, 0)\n",
    "            angle_deg = np.degrees(angle_rad)\n",
    "            angle_deg = angle_deg % 360 \n",
    "                            \n",
    "            # Getting image center\n",
    "            (h, w) = img_input.shape[:2]\n",
    "            image_center = (w // 2, h // 2)\n",
    "            \n",
    "            # rotation matrix\n",
    "            M = cv2.getRotationMatrix2D(image_center, angle_deg, 1.0)\n",
    "            \n",
    "            # Perform the rotation\n",
    "            rotated = cv2.warpAffine(copy_image, M, (w, h))\n",
    "            #show_image(rotated, \"Rotated image\")\n",
    "        \n",
    "        # preprocessing - cycle2\n",
    "    \n",
    "        gray_2 = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY) \n",
    "        _, thresh_2 = cv2.threshold(gray_2,220, 255, cv2.THRESH_BINARY)             \n",
    "        edges_2 = cv2.Canny(thresh_2, 100, 200)\n",
    "                      \n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        \n",
    "        dilated_2 = cv2.dilate(edges_2, kernel, iterations=2)\n",
    "        eroded_2 = cv2.erode(dilated_2, kernel, iterations=1)\n",
    "        \n",
    "        contours, _ = cv2.findContours(eroded_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Process only if we have at least one valid contour\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            cv2.rectangle(rotated, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "            \n",
    "            \n",
    "            if area > 500:  # Filter small areas\n",
    "                                       \n",
    "                cropped = rotated[y:y+h, x:x+w]                       \n",
    "                resize_img = cv2.resize(cropped, (224, 224))\n",
    "                \n",
    "                # preprocessing - cycle3\n",
    "                \n",
    "                gray_3 = cv2.cvtColor(resize_img, cv2.COLOR_BGR2GRAY)                       \n",
    "                _, thresh_3 = cv2.threshold(gray_3,220, 255, cv2.THRESH_BINARY)\n",
    "                edges_3 = cv2.Canny(thresh_3, 100, 200)\n",
    "                \n",
    "                kernel = np.ones((5, 5), np.uint8)\n",
    "                \n",
    "                dilated_3 = cv2.dilate(edges_3, kernel, iterations=2)\n",
    "                eroded_3 = cv2.erode(dilated_3, kernel, iterations=1)\n",
    "                \n",
    "                contours, _ = cv2.findContours(thresh_3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "                for cnt in contours:\n",
    "                    area2 = cv2.contourArea(cnt)\n",
    "                    \n",
    "                    \n",
    "                    if area2> 19000 and area2 <30000:\n",
    "                                            \n",
    "                        background = np.ones((224, 224, 3), dtype=np.uint8) * 255  \n",
    "                        \n",
    "                        # Creating mask for the oval contour\n",
    "                        mask = np.zeros_like(resize_img)\n",
    "                        cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                                                       \n",
    "                        # Applying mask to extract oval region\n",
    "                        masked = cv2.bitwise_and(resize_img, mask)\n",
    "                        \n",
    "                        # Crop the contour region\n",
    "                        x, y, w, h = cv2.boundingRect(cnt)\n",
    "                        cropped = masked[y:y+h, x:x+w]\n",
    "                        \n",
    "                        # Get shape of cropped region\n",
    "                        ch, cw = cropped.shape[:2]\n",
    "        \n",
    "                        target_size = 224\n",
    "                        # Compute offsets to center it\n",
    "                        x_offset = (target_size - cw) // 2\n",
    "                        y_offset = (target_size - ch) // 2\n",
    "                        \n",
    "                        # Pasting cropped oval into the center of white background\n",
    "                        background[y_offset:y_offset+ch, x_offset:x_offset+cw] = cropped\n",
    "    \n",
    "                        \n",
    "                        #final_crop=cv2.cvtColor(background, cv2.COLOR_BGR2RGB)\n",
    "                        gray_final = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "                        _, thresh_final = cv2.threshold(gray_final,226, 255, cv2.THRESH_BINARY)\n",
    "                        #show_image(thresh_final,\"final - image\",cmap=\"grey\")\n",
    "    \n",
    "                        # Resize to final model input size\n",
    "                        final_img = cv2.resize(thresh_final, (img_size, img_size))\n",
    "        \n",
    "                        # Preprocess the image\n",
    "                        img_processed = preprocess_input(final_img.astype(np.float32))\n",
    "                        img_array = np.expand_dims(img_processed, axis=0)\n",
    "    \n",
    "                        predictions = model.predict(img_array, verbose=0)\n",
    "                        class_id = np.argmax(predictions)\n",
    "                        confidence = np.max(predictions)\n",
    "                    \n",
    "                        # Draw rectangle and label\n",
    "                        label = f\"{categories[class_id]}\"#\" ({confidence * 100:.2f}%)\"\n",
    "                        cv2.rectangle(images, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                        cv2.putText(images, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "    #cv2.putText(contour_image,label,(10, 30),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),2,cv2.LINE_AA)\n",
    "                        \n",
    "    # Show video feed\n",
    "    cv2.imshow(\"Live Detection\", contour_image)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d0333-7e70-4914-b6d5-d24bab06bed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d37881-9e2d-48a8-995c-908dad351c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
